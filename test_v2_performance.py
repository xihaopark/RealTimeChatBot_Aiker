#!/usr/bin/env python3
"""
VTX AI Phone System V2 æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°æ¶æ„çš„æ€§èƒ½å’Œç¨³å®šæ€§
"""

import time
import threading
import logging
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'aiker_v2'))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results = {}
        
    def test_tts_performance(self, iterations: int = 10) -> Dict[str, Any]:
        """æµ‹è¯•TTSæ€§èƒ½"""
        logger.info(f"ğŸµ Testing TTS performance ({iterations} iterations)...")
        
        try:
            from tts_service import PiperTTSService
            
            tts = PiperTTSService()
            if not tts.is_available():
                return {"error": "TTS service not available"}
            
            # æµ‹è¯•ç”¨ä¾‹
            test_texts = [
                "æ‚¨å¥½ï¼Œæ¬¢è¿è‡´ç”µOneSuiteå®¢æœä¸­å¿ƒã€‚",
                "è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "æ„Ÿè°¢æ‚¨çš„æ¥ç”µï¼Œç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ã€‚",
                "Hello, welcome to OneSuite customer service.",
                "How may I assist you today?"
            ]
            
            timings = []
            char_counts = []
            
            for i in range(iterations):
                text = test_texts[i % len(test_texts)]
                char_counts.append(len(text))
                
                start_time = time.time()
                audio_data = tts.synthesize_for_rtp(text, 'zh' if i % 2 == 0 else 'en')
                end_time = time.time()
                
                if audio_data:
                    timings.append(end_time - start_time)
                else:
                    logger.warning(f"TTS failed for iteration {i}")
            
            if not timings:
                return {"error": "All TTS attempts failed"}
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            avg_time = statistics.mean(timings)
            avg_chars = statistics.mean(char_counts)
            chars_per_second = avg_chars / avg_time
            
            result = {
                "service": "Piper TTS",
                "iterations": len(timings),
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "chars_per_second": round(chars_per_second, 1),
                "success_rate": len(timings) / iterations
            }
            
            logger.info(f"âœ… TTS Performance: {avg_time:.3f}s avg, {chars_per_second:.1f} chars/s")
            return result
            
        except Exception as e:
            logger.error(f"TTS test failed: {e}")
            return {"error": str(e)}
    
    def test_stt_performance(self, iterations: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•STTæ€§èƒ½"""
        logger.info(f"ğŸ¤ Testing STT performance ({iterations} iterations)...")
        
        try:
            from stt_service import VoskSTTService
            
            stt = VoskSTTService()
            if not stt.is_available():
                return {"error": "STT service not available"}
            
            # åˆ›å»ºæµ‹è¯•éŸ³é¢‘æ•°æ® (æ¨¡æ‹ŸPCMéŸ³é¢‘)
            import numpy as np
            
            sample_rate = 8000
            duration = 2.0  # 2ç§’éŸ³é¢‘
            samples = int(sample_rate * duration)
            
            # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘ (æ­£å¼¦æ³¢)
            frequency = 440  # A4éŸ³ç¬¦
            t = np.linspace(0, duration, samples, False)
            audio_wave = np.sin(frequency * 2 * np.pi * t)
            audio_pcm = (audio_wave * 32767).astype(np.int16).tobytes()
            
            timings = []
            success_count = 0
            
            for i in range(iterations):
                start_time = time.time()
                
                # æ¨¡æ‹ŸéŸ³é¢‘å—å¤„ç†
                chunk_size = 1600  # 0.1ç§’çš„éŸ³é¢‘å—
                for j in range(0, len(audio_pcm), chunk_size):
                    chunk = audio_pcm[j:j+chunk_size]
                    result = stt.process_audio_chunk(chunk, 'zh')
                    if result:
                        success_count += 1
                        break
                
                end_time = time.time()
                timings.append(end_time - start_time)
            
            avg_time = statistics.mean(timings)
            
            result = {
                "service": "Vosk STT",
                "iterations": iterations,
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "success_rate": success_count / iterations,
                "audio_duration": duration
            }
            
            logger.info(f"âœ… STT Performance: {avg_time:.3f}s avg processing time")
            return result
            
        except Exception as e:
            logger.error(f"STT test failed: {e}")
            return {"error": str(e)}
    
    def test_llm_performance(self, iterations: int = 10) -> Dict[str, Any]:
        """æµ‹è¯•LLMæ€§èƒ½"""
        logger.info(f"ğŸ§  Testing LLM performance ({iterations} iterations)...")
        
        try:
            from llm_service import LlamaCppLLMService
            
            llm = LlamaCppLLMService()
            if not llm.is_available():
                return {"error": "LLM service not available"}
            
            # æµ‹è¯•é—®é¢˜
            test_questions = [
                "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ ä»¬çš„æœåŠ¡ã€‚",
                "è¥ä¸šæ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
                "å¦‚ä½•è”ç³»æŠ€æœ¯æ”¯æŒï¼Ÿ",
                "äº§å“ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ",
                "å¯ä»¥é€€æ¢è´§å—ï¼Ÿ"
            ]
            
            timings = []
            token_counts = []
            success_count = 0
            
            for i in range(iterations):
                question = test_questions[i % len(test_questions)]
                conversation_id = f"test_{i}"
                
                start_time = time.time()
                response = llm.generate_response(question, conversation_id)
                end_time = time.time()
                
                if response and "æŠ±æ­‰" not in response:
                    success_count += 1
                    timings.append(end_time - start_time)
                    token_counts.append(len(response))
                else:
                    logger.warning(f"LLM failed for iteration {i}")
            
            if not timings:
                return {"error": "All LLM attempts failed"}
            
            avg_time = statistics.mean(timings)
            avg_tokens = statistics.mean(token_counts)
            tokens_per_second = avg_tokens / avg_time
            
            result = {
                "service": "Llama.cpp LLM",
                "iterations": len(timings),
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "tokens_per_second": round(tokens_per_second, 1),
                "avg_response_length": round(avg_tokens, 1),
                "success_rate": success_count / iterations
            }
            
            logger.info(f"âœ… LLM Performance: {avg_time:.3f}s avg, {tokens_per_second:.1f} tokens/s")
            return result
            
        except Exception as e:
            logger.error(f"LLM test failed: {e}")
            return {"error": str(e)}
    
    def test_concurrent_load(self, concurrent_calls: int = 5, duration: int = 30) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è´Ÿè½½"""
        logger.info(f"âš¡ Testing concurrent load ({concurrent_calls} calls for {duration}s)...")
        
        def simulate_call(call_id: int) -> Dict[str, Any]:
            """æ¨¡æ‹Ÿå•ä¸ªé€šè¯"""
            start_time = time.time()
            
            try:
                from tts_service import PiperTTSService
                from llm_service import LlamaCppLLMService
                
                tts = PiperTTSService()
                llm = LlamaCppLLMService()
                
                operations = 0
                errors = 0
                
                end_time = start_time + duration
                
                while time.time() < end_time:
                    try:
                        # æ¨¡æ‹Ÿå¯¹è¯æµç¨‹
                        question = f"è¿™æ˜¯ç¬¬{operations + 1}ä¸ªé—®é¢˜"
                        response = llm.generate_response(question, f"load_test_{call_id}")
                        
                        if response:
                            audio = tts.synthesize_for_rtp(response, 'zh')
                            if audio:
                                operations += 1
                            else:
                                errors += 1
                        else:
                            errors += 1
                        
                        # æ¨¡æ‹Ÿé€šè¯é—´éš”
                        time.sleep(1)
                        
                    except Exception as e:
                        errors += 1
                        logger.debug(f"Call {call_id} error: {e}")
                
                actual_duration = time.time() - start_time
                
                return {
                    "call_id": call_id,
                    "operations": operations,
                    "errors": errors,
                    "duration": actual_duration,
                    "ops_per_second": operations / actual_duration if actual_duration > 0 else 0
                }
                
            except Exception as e:
                return {
                    "call_id": call_id,
                    "error": str(e)
                }
        
        # å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=concurrent_calls) as executor:
            futures = [executor.submit(simulate_call, i) for i in range(concurrent_calls)]
            call_results = [future.result() for future in futures]
        
        # ç»Ÿè®¡ç»“æœ
        successful_calls = [r for r in call_results if "error" not in r]
        total_operations = sum(r.get("operations", 0) for r in successful_calls)
        total_errors = sum(r.get("errors", 0) for r in successful_calls)
        
        if successful_calls:
            avg_ops_per_second = statistics.mean([r["ops_per_second"] for r in successful_calls])
        else:
            avg_ops_per_second = 0
        
        result = {
            "concurrent_calls": concurrent_calls,
            "duration_seconds": duration,
            "successful_calls": len(successful_calls),
            "total_operations": total_operations,
            "total_errors": total_errors,
            "avg_ops_per_second": round(avg_ops_per_second, 2),
            "success_rate": len(successful_calls) / concurrent_calls
        }
        
        logger.info(f"âœ… Concurrent Load: {len(successful_calls)}/{concurrent_calls} calls successful")
        return result
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        logger.info("ğŸ’¾ Testing memory usage...")
        
        try:
            import psutil
            
            process = psutil.Process()
            
            # è·å–å½“å‰å†…å­˜ä½¿ç”¨
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            
            # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
            system_memory = psutil.virtual_memory()
            
            result = {
                "process_memory_mb": round(memory_info.rss / 1024 / 1024, 1),
                "process_memory_percent": round(memory_percent, 1),
                "system_memory_total_gb": round(system_memory.total / 1024 / 1024 / 1024, 1),
                "system_memory_available_gb": round(system_memory.available / 1024 / 1024 / 1024, 1),
                "system_memory_percent": system_memory.percent
            }
            
            logger.info(f"âœ… Memory Usage: {result['process_memory_mb']}MB ({result['process_memory_percent']}%)")
            return result
            
        except Exception as e:
            logger.error(f"Memory test failed: {e}")
            return {"error": str(e)}
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ Starting comprehensive performance tests...")
        
        start_time = time.time()
        
        self.results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {}
        }
        
        # ä¾æ¬¡æ‰§è¡Œå„é¡¹æµ‹è¯•
        test_functions = [
            ("memory_usage", self.test_memory_usage),
            ("tts_performance", self.test_tts_performance),
            ("stt_performance", self.test_stt_performance),
            ("llm_performance", self.test_llm_performance),
            ("concurrent_load", lambda: self.test_concurrent_load(3, 15))  # å‡å°‘æµ‹è¯•æ—¶é—´
        ]
        
        for test_name, test_func in test_functions:
            logger.info(f"\n--- Running {test_name} test ---")
            try:
                self.results["tests"][test_name] = test_func()
            except Exception as e:
                logger.error(f"Test {test_name} failed: {e}")
                self.results["tests"][test_name] = {"error": str(e)}
        
        total_time = time.time() - start_time
        self.results["total_test_time"] = round(total_time, 2)
        
        logger.info(f"\nâœ… All tests completed in {total_time:.2f}s")
        return self.results
    
    def save_results(self, filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"performance_test_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š Test results saved to {filename}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.results:
            logger.warning("No test results available")
            return
        
        print("\n" + "="*60)
        print("ğŸ† VTX AI Phone System V2 Performance Test Summary")
        print("="*60)
        
        tests = self.results.get("tests", {})
        
        for test_name, result in tests.items():
            if "error" in result:
                print(f"âŒ {test_name}: {result['error']}")
            else:
                print(f"âœ… {test_name}:")
                
                if test_name == "tts_performance":
                    print(f"   Average time: {result.get('avg_time_seconds', 'N/A')}s")
                    print(f"   Speed: {result.get('chars_per_second', 'N/A')} chars/s")
                    
                elif test_name == "llm_performance":
                    print(f"   Average time: {result.get('avg_time_seconds', 'N/A')}s")
                    print(f"   Speed: {result.get('tokens_per_second', 'N/A')} tokens/s")
                    
                elif test_name == "concurrent_load":
                    print(f"   Successful calls: {result.get('successful_calls', 'N/A')}")
                    print(f"   Operations/second: {result.get('avg_ops_per_second', 'N/A')}")
                    
                elif test_name == "memory_usage":
                    print(f"   Process memory: {result.get('process_memory_mb', 'N/A')}MB")
                    print(f"   System memory: {result.get('system_memory_percent', 'N/A')}%")
        
        print(f"\nTotal test time: {self.results.get('total_test_time', 'N/A')}s")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª VTX AI Phone System V2 Performance Testing")
    print("This will test TTS, STT, LLM, and concurrent performance.")
    print()
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = PerformanceTestSuite()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        results = test_suite.run_all_tests()
        
        # ä¿å­˜ç»“æœ
        test_suite.save_results()
        
        # æ‰“å°æ‘˜è¦
        test_suite.print_summary()
        
    except KeyboardInterrupt:
        logger.info("Testing interrupted by user")
    except Exception as e:
        logger.error(f"Testing failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())