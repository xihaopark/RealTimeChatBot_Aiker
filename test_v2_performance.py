#!/usr/bin/env python3
"""
VTX AI Phone System V2 ÊÄßËÉΩÊµãËØïËÑöÊú¨
ÊµãËØïÊñ∞Êû∂ÊûÑÁöÑÊÄßËÉΩÂíåÁ®≥ÂÆöÊÄß
"""

import time
import threading
import logging
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import sys
import os

# Ê∑ªÂä†È°πÁõÆË∑ØÂæÑ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'aiker_v2'))

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """ÊÄßËÉΩÊµãËØïÂ•ó‰ª∂"""
    
    def __init__(self):
        self.results = {}
        
    def test_tts_performance(self, iterations: int = 10) -> Dict[str, Any]:
        """ÊµãËØïTTSÊÄßËÉΩ"""
        logger.info(f"üéµ Testing TTS performance ({iterations} iterations)...")
        
        try:
            from tts_service import PiperTTSService
            
            tts = PiperTTSService()
            if not tts.is_available():
                return {"error": "TTS service not available"}
            
            # ÊµãËØïÁî®‰æã
            test_texts = [
                "ÊÇ®Â•ΩÔºåÊ¨¢ËøéËá¥ÁîµOneSuiteÂÆ¢Êúç‰∏≠ÂøÉ„ÄÇ",
                "ËØ∑ÈóÆÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©ÊÇ®ÁöÑÂêóÔºü",
                "ÊÑüË∞¢ÊÇ®ÁöÑÊù•ÁîµÔºåÁ•ùÊÇ®ÁîüÊ¥ªÊÑâÂø´„ÄÇ",
                "Hello, welcome to OneSuite customer service.",
                "How may I assist you today?"
            ]
            
            timings = []
            char_counts = []
            
            for i in range(iterations):
                text = test_texts[i % len(test_texts)]
                char_counts.append(len(text))
                
                start_time = time.time()
                audio_data = tts.synthesize_for_rtp(text, 'zh' if i % 2 == 0 else 'en')
                end_time = time.time()
                
                if audio_data:
                    timings.append(end_time - start_time)
                else:
                    logger.warning(f"TTS failed for iteration {i}")
            
            if not timings:
                return {"error": "All TTS attempts failed"}
            
            # ËÆ°ÁÆóÊÄßËÉΩÊåáÊ†á
            avg_time = statistics.mean(timings)
            avg_chars = statistics.mean(char_counts)
            chars_per_second = avg_chars / avg_time
            
            result = {
                "service": "Piper TTS",
                "iterations": len(timings),
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "chars_per_second": round(chars_per_second, 1),
                "success_rate": len(timings) / iterations
            }
            
            logger.info(f"‚úÖ TTS Performance: {avg_time:.3f}s avg, {chars_per_second:.1f} chars/s")
            return result
            
        except Exception as e:
            logger.error(f"TTS test failed: {e}")
            return {"error": str(e)}
    
    def test_stt_performance(self, iterations: int = 5) -> Dict[str, Any]:
        """ÊµãËØïSTTÊÄßËÉΩ"""
        logger.info(f"üé§ Testing STT performance ({iterations} iterations)...")
        
        try:
            from stt_service import VoskSTTService
            
            stt = VoskSTTService()
            if not stt.is_available():
                return {"error": "STT service not available"}
            
            # ÂàõÂª∫ÊµãËØïÈü≥È¢ëÊï∞ÊçÆ (Ê®°ÊãüPCMÈü≥È¢ë)
            import numpy as np
            
            sample_rate = 8000
            duration = 2.0  # 2ÁßíÈü≥È¢ë
            samples = int(sample_rate * duration)
            
            # ÁîüÊàêÊµãËØïÈü≥È¢ë (Ê≠£Âº¶Ê≥¢)
            frequency = 440  # A4Èü≥Á¨¶
            t = np.linspace(0, duration, samples, False)
            audio_wave = np.sin(frequency * 2 * np.pi * t)
            audio_pcm = (audio_wave * 32767).astype(np.int16).tobytes()
            
            timings = []
            success_count = 0
            
            for i in range(iterations):
                start_time = time.time()
                
                # Ê®°ÊãüÈü≥È¢ëÂùóÂ§ÑÁêÜ
                chunk_size = 1600  # 0.1ÁßíÁöÑÈü≥È¢ëÂùó
                for j in range(0, len(audio_pcm), chunk_size):
                    chunk = audio_pcm[j:j+chunk_size]
                    result = stt.process_audio_chunk(chunk, 'zh')
                    if result:
                        success_count += 1
                        break
                
                end_time = time.time()
                timings.append(end_time - start_time)
            
            avg_time = statistics.mean(timings)
            
            result = {
                "service": "Vosk STT",
                "iterations": iterations,
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "success_rate": success_count / iterations,
                "audio_duration": duration
            }
            
            logger.info(f"‚úÖ STT Performance: {avg_time:.3f}s avg processing time")
            return result
            
        except Exception as e:
            logger.error(f"STT test failed: {e}")
            return {"error": str(e)}
    
    def test_llm_performance(self, iterations: int = 10) -> Dict[str, Any]:
        """ÊµãËØïLLMÊÄßËÉΩ"""
        logger.info(f"üß† Testing LLM performance ({iterations} iterations)...")
        
        try:
            from llm_service import LlamaCppLLMService
            
            llm = LlamaCppLLMService()
            if not llm.is_available():
                return {"error": "LLM service not available"}
            
            # ÊµãËØïÈóÆÈ¢ò
            test_questions = [
                "‰Ω†Â•ΩÔºåËØ∑‰ªãÁªç‰∏Ä‰∏ã‰Ω†‰ª¨ÁöÑÊúçÂä°„ÄÇ",
                "Ëê•‰∏öÊó∂Èó¥ÊòØ‰ªÄ‰πàÊó∂ÂÄôÔºü",
                "Â¶Ç‰ΩïËÅîÁ≥ªÊäÄÊúØÊîØÊåÅÔºü",
                "‰∫ßÂìÅ‰ª∑Ê†ºÊòØÂ§öÂ∞ëÔºü",
                "ÂèØ‰ª•ÈÄÄÊç¢Ë¥ßÂêóÔºü"
            ]
            
            timings = []
            token_counts = []
            success_count = 0
            
            for i in range(iterations):
                question = test_questions[i % len(test_questions)]
                conversation_id = f"test_{i}"
                
                start_time = time.time()
                response = llm.generate_response(question, conversation_id)
                end_time = time.time()
                
                if response and "Êä±Ê≠â" not in response:
                    success_count += 1
                    timings.append(end_time - start_time)
                    token_counts.append(len(response))
                else:
                    logger.warning(f"LLM failed for iteration {i}")
            
            if not timings:
                return {"error": "All LLM attempts failed"}
            
            avg_time = statistics.mean(timings)
            avg_tokens = statistics.mean(token_counts)
            tokens_per_second = avg_tokens / avg_time
            
            result = {
                "service": "Llama.cpp LLM",
                "iterations": len(timings),
                "avg_time_seconds": round(avg_time, 3),
                "min_time_seconds": round(min(timings), 3),
                "max_time_seconds": round(max(timings), 3),
                "tokens_per_second": round(tokens_per_second, 1),
                "avg_response_length": round(avg_tokens, 1),
                "success_rate": success_count / iterations
            }
            
            logger.info(f"‚úÖ LLM Performance: {avg_time:.3f}s avg, {tokens_per_second:.1f} tokens/s")
            return result
            
        except Exception as e:
            logger.error(f"LLM test failed: {e}")
            return {"error": str(e)}
    
    def test_concurrent_load(self, concurrent_calls: int = 5, duration: int = 30) -> Dict[str, Any]:
        """ÊµãËØïÂπ∂ÂèëË¥üËΩΩ"""
        logger.info(f"‚ö° Testing concurrent load ({concurrent_calls} calls for {duration}s)...")
        
        def simulate_call(call_id: int) -> Dict[str, Any]:
            """Ê®°ÊãüÂçï‰∏™ÈÄöËØù"""
            start_time = time.time()
            
            try:
                from tts_service import PiperTTSService
                from llm_service import LlamaCppLLMService
                
                tts = PiperTTSService()
                llm = LlamaCppLLMService()
                
                operations = 0
                errors = 0
                
                end_time = start_time + duration
                
                while time.time() < end_time:
                    try:
                        # Ê®°ÊãüÂØπËØùÊµÅÁ®ã
                        question = f"ËøôÊòØÁ¨¨{operations + 1}‰∏™ÈóÆÈ¢ò"
                        response = llm.generate_response(question, f"load_test_{call_id}")
                        
                        if response:
                            audio = tts.synthesize_for_rtp(response, 'zh')
                            if audio:
                                operations += 1
                            else:
                                errors += 1
                        else:
                            errors += 1
                        
                        # Ê®°ÊãüÈÄöËØùÈó¥Èöî
                        time.sleep(1)
                        
                    except Exception as e:
                        errors += 1
                        logger.debug(f"Call {call_id} error: {e}")
                
                actual_duration = time.time() - start_time
                
                return {
                    "call_id": call_id,
                    "operations": operations,
                    "errors": errors,
                    "duration": actual_duration,
                    "ops_per_second": operations / actual_duration if actual_duration > 0 else 0
                }
                
            except Exception as e:
                return {
                    "call_id": call_id,
                    "error": str(e)
                }
        
        # Âπ∂ÂèëÊâßË°å
        with ThreadPoolExecutor(max_workers=concurrent_calls) as executor:
            futures = [executor.submit(simulate_call, i) for i in range(concurrent_calls)]
            call_results = [future.result() for future in futures]
        
        # ÁªüËÆ°ÁªìÊûú
        successful_calls = [r for r in call_results if "error" not in r]
        total_operations = sum(r.get("operations", 0) for r in successful_calls)
        total_errors = sum(r.get("errors", 0) for r in successful_calls)
        
        if successful_calls:
            avg_ops_per_second = statistics.mean([r["ops_per_second"] for r in successful_calls])
        else:
            avg_ops_per_second = 0
        
        result = {
            "concurrent_calls": concurrent_calls,
            "duration_seconds": duration,
            "successful_calls": len(successful_calls),
            "total_operations": total_operations,
            "total_errors": total_errors,
            "avg_ops_per_second": round(avg_ops_per_second, 2),
            "success_rate": len(successful_calls) / concurrent_calls
        }
        
        logger.info(f"‚úÖ Concurrent Load: {len(successful_calls)}/{concurrent_calls} calls successful")
        return result
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """ÊµãËØïÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ"""
        logger.info("üíæ Testing memory usage...")
        
        try:
            import psutil
            
            process = psutil.Process()
            
            # Ëé∑ÂèñÂΩìÂâçÂÜÖÂ≠ò‰ΩøÁî®
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            
            # Ëé∑ÂèñÁ≥ªÁªüÂÜÖÂ≠ò‰ø°ÊÅØ
            system_memory = psutil.virtual_memory()
            
            result = {
                "process_memory_mb": round(memory_info.rss / 1024 / 1024, 1),
                "process_memory_percent": round(memory_percent, 1),
                "system_memory_total_gb": round(system_memory.total / 1024 / 1024 / 1024, 1),
                "system_memory_available_gb": round(system_memory.available / 1024 / 1024 / 1024, 1),
                "system_memory_percent": system_memory.percent
            }
            
            logger.info(f"‚úÖ Memory Usage: {result['process_memory_mb']}MB ({result['process_memory_percent']}%)")
            return result
            
        except Exception as e:
            logger.error(f"Memory test failed: {e}")
            return {"error": str(e)}
    
    def run_all_tests(self) -> Dict[str, Any]:
        """ËøêË°åÊâÄÊúâÊÄßËÉΩÊµãËØï"""
        logger.info("üöÄ Starting comprehensive performance tests...")
        
        start_time = time.time()
        
        self.results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {}
        }
        
        # ‰æùÊ¨°ÊâßË°åÂêÑÈ°πÊµãËØï
        test_functions = [
            ("memory_usage", self.test_memory_usage),
            ("tts_performance", self.test_tts_performance),
            ("stt_performance", self.test_stt_performance),
            ("llm_performance", self.test_llm_performance),
            ("concurrent_load", lambda: self.test_concurrent_load(3, 15))  # ÂáèÂ∞ëÊµãËØïÊó∂Èó¥
        ]
        
        for test_name, test_func in test_functions:
            logger.info(f"\n--- Running {test_name} test ---")
            try:
                self.results["tests"][test_name] = test_func()
            except Exception as e:
                logger.error(f"Test {test_name} failed: {e}")
                self.results["tests"][test_name] = {"error": str(e)}
        
        total_time = time.time() - start_time
        self.results["total_test_time"] = round(total_time, 2)
        
        logger.info(f"\n‚úÖ All tests completed in {total_time:.2f}s")
        return self.results
    
    def save_results(self, filename: str = None):
        """‰øùÂ≠òÊµãËØïÁªìÊûú"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"performance_test_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìä Test results saved to {filename}")
    
    def print_summary(self):
        """ÊâìÂç∞ÊµãËØïÊëòË¶Å"""
        if not self.results:
            logger.warning("No test results available")
            return
        
        print("\n" + "="*60)
        print("üèÜ VTX AI Phone System V2 Performance Test Summary")
        print("="*60)
        
        tests = self.results.get("tests", {})
        
        for test_name, result in tests.items():
            if "error" in result:
                print(f"‚ùå {test_name}: {result['error']}")
            else:
                print(f"‚úÖ {test_name}:")
                
                if test_name == "tts_performance":
                    print(f"   Average time: {result.get('avg_time_seconds', 'N/A')}s")
                    print(f"   Speed: {result.get('chars_per_second', 'N/A')} chars/s")
                    
                elif test_name == "llm_performance":
                    print(f"   Average time: {result.get('avg_time_seconds', 'N/A')}s")
                    print(f"   Speed: {result.get('tokens_per_second', 'N/A')} tokens/s")
                    
                elif test_name == "concurrent_load":
                    print(f"   Successful calls: {result.get('successful_calls', 'N/A')}")
                    print(f"   Operations/second: {result.get('avg_ops_per_second', 'N/A')}")
                    
                elif test_name == "memory_usage":
                    print(f"   Process memory: {result.get('process_memory_mb', 'N/A')}MB")
                    print(f"   System memory: {result.get('system_memory_percent', 'N/A')}%")
        
        print(f"\nTotal test time: {self.results.get('total_test_time', 'N/A')}s")
        print("="*60)


def main():
    """‰∏ªÂáΩÊï∞"""
    print("üß™ VTX AI Phone System V2 Performance Testing")
    print("This will test TTS, STT, LLM, and concurrent performance.")
    print()
    
    # ÂàõÂª∫ÊµãËØïÂ•ó‰ª∂
    test_suite = PerformanceTestSuite()
    
    try:
        # ËøêË°åÊâÄÊúâÊµãËØï
        results = test_suite.run_all_tests()
        
        # ‰øùÂ≠òÁªìÊûú
        test_suite.save_results()
        
        # ÊâìÂç∞ÊëòË¶Å
        test_suite.print_summary()
        
    except KeyboardInterrupt:
        logger.info("Testing interrupted by user")
    except Exception as e:
        logger.error(f"Testing failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())