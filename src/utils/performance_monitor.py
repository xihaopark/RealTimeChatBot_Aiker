#!/usr/bin/env python3
"""
VTX AI Phone System - æ€§èƒ½ç›‘æ§å·¥å…·
"""

import time
import statistics
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from collections import deque
import logging

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    response_times: List[float] = field(default_factory=list)
    stt_latency: List[float] = field(default_factory=list)
    tts_latency: List[float] = field(default_factory=list)
    llm_latency: List[float] = field(default_factory=list)
    error_count: int = 0
    total_requests: int = 0
    start_time: float = field(default_factory=time.time)
    
    def add_response_time(self, response_time: float):
        """æ·»åŠ å“åº”æ—¶é—´"""
        self.response_times.append(response_time)
        self.total_requests += 1
    
    def add_stt_latency(self, latency: float):
        """æ·»åŠ STTå»¶è¿Ÿ"""
        self.stt_latency.append(latency)
    
    def add_tts_latency(self, latency: float):
        """æ·»åŠ TTSå»¶è¿Ÿ"""
        self.tts_latency.append(latency)
    
    def add_llm_latency(self, latency: float):
        """æ·»åŠ LLMå»¶è¿Ÿ"""
        self.llm_latency.append(latency)
    
    def increment_error(self):
        """å¢åŠ é”™è¯¯è®¡æ•°"""
        self.error_count += 1
    
    def get_stats(self) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_requests': self.total_requests,
            'error_count': self.error_count,
            'error_rate': self.error_count / max(self.total_requests, 1),
            'uptime': time.time() - self.start_time
        }
        
        if self.response_times:
            stats.update({
                'avg_response_time': statistics.mean(self.response_times),
                'min_response_time': min(self.response_times),
                'max_response_time': max(self.response_times),
                'median_response_time': statistics.median(self.response_times)
            })
        
        if self.stt_latency:
            stats['avg_stt_latency'] = statistics.mean(self.stt_latency)
        
        if self.tts_latency:
            stats['avg_tts_latency'] = statistics.mean(self.tts_latency)
        
        if self.llm_latency:
            stats['avg_llm_latency'] = statistics.mean(self.llm_latency)
        
        return stats


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, max_history: int = 1000):
        self.metrics = PerformanceMetrics()
        self.max_history = max_history
        self.recent_response_times = deque(maxlen=max_history)
        self.target_latency = 0.8  # ç›®æ ‡å»¶è¿Ÿ800ms
        
    def record_response_time(self, response_time: float):
        """è®°å½•å“åº”æ—¶é—´"""
        self.metrics.add_response_time(response_time)
        self.recent_response_times.append(response_time)
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç›®æ ‡å»¶è¿Ÿ
        if response_time > self.target_latency:
            logger.warning(f"âš ï¸ å“åº”æ—¶é—´ {response_time:.2f}s è¶…è¿‡ç›®æ ‡ {self.target_latency}s")
    
    def record_stt_latency(self, latency: float):
        """è®°å½•STTå»¶è¿Ÿ"""
        self.metrics.add_stt_latency(latency)
    
    def record_tts_latency(self, latency: float):
        """è®°å½•TTSå»¶è¿Ÿ"""
        self.metrics.add_tts_latency(latency)
    
    def record_llm_latency(self, latency: float):
        """è®°å½•LLMå»¶è¿Ÿ"""
        self.metrics.add_llm_latency(latency)
    
    def record_error(self):
        """è®°å½•é”™è¯¯"""
        self.metrics.increment_error()
    
    def get_current_stats(self) -> Dict[str, float]:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        return self.metrics.get_stats()
    
    def get_recent_performance(self, window_size: int = 10) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡"""
        if len(self.recent_response_times) < window_size:
            return {}
        
        recent_times = list(self.recent_response_times)[-window_size:]
        
        return {
            'recent_avg_response_time': statistics.mean(recent_times),
            'recent_min_response_time': min(recent_times),
            'recent_max_response_time': max(recent_times),
            'recent_median_response_time': statistics.median(recent_times)
        }
    
    def is_performance_acceptable(self) -> bool:
        """æ£€æŸ¥æ€§èƒ½æ˜¯å¦å¯æ¥å—"""
        if not self.recent_response_times:
            return True
        
        recent_avg = statistics.mean(list(self.recent_response_times)[-10:])
        return recent_avg <= self.target_latency
    
    def print_performance_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        stats = self.get_current_stats()
        recent_stats = self.get_recent_performance()
        
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        print("=" * 60)
        print(f"æ€»è¯·æ±‚æ•°: {stats.get('total_requests', 0)}")
        print(f"é”™è¯¯æ•°: {stats.get('error_count', 0)}")
        print(f"é”™è¯¯ç‡: {stats.get('error_rate', 0):.2%}")
        print(f"è¿è¡Œæ—¶é—´: {stats.get('uptime', 0):.1f}ç§’")
        
        if 'avg_response_time' in stats:
            print(f"\nå“åº”æ—¶é—´ç»Ÿè®¡:")
            print(f"  å¹³å‡: {stats['avg_response_time']:.3f}s")
            print(f"  ä¸­ä½æ•°: {stats['median_response_time']:.3f}s")
            print(f"  æœ€å°: {stats['min_response_time']:.3f}s")
            print(f"  æœ€å¤§: {stats['max_response_time']:.3f}s")
        
        if 'avg_stt_latency' in stats:
            print(f"STTå¹³å‡å»¶è¿Ÿ: {stats['avg_stt_latency']:.3f}s")
        
        if 'avg_tts_latency' in stats:
            print(f"TTSå¹³å‡å»¶è¿Ÿ: {stats['avg_tts_latency']:.3f}s")
        
        if 'avg_llm_latency' in stats:
            print(f"LLMå¹³å‡å»¶è¿Ÿ: {stats['avg_llm_latency']:.3f}s")
        
        if recent_stats:
            print(f"\næœ€è¿‘10æ¬¡å“åº”æ—¶é—´:")
            print(f"  å¹³å‡: {recent_stats['recent_avg_response_time']:.3f}s")
            print(f"  ä¸­ä½æ•°: {recent_stats['recent_median_response_time']:.3f}s")
        
        # æ€§èƒ½è¯„ä¼°
        if self.is_performance_acceptable():
            print(f"\nâœ… æ€§èƒ½çŠ¶æ€: è‰¯å¥½ (ç›®æ ‡: <{self.target_latency}s)")
        else:
            print(f"\nâš ï¸ æ€§èƒ½çŠ¶æ€: éœ€è¦ä¼˜åŒ– (ç›®æ ‡: <{self.target_latency}s)")
        
        print("=" * 60)
    
    def reset(self):
        """é‡ç½®ç›‘æ§æ•°æ®"""
        self.metrics = PerformanceMetrics()
        self.recent_response_times.clear()
        logger.info("æ€§èƒ½ç›‘æ§æ•°æ®å·²é‡ç½®")
    
    def export_metrics(self) -> Dict:
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡"""
        return {
            'metrics': self.metrics.__dict__,
            'recent_performance': self.get_recent_performance(),
            'performance_acceptable': self.is_performance_acceptable(),
            'target_latency': self.target_latency
        }


# å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹
performance_monitor = PerformanceMonitor() 