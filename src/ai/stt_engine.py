"""
è¯­éŸ³è¯†åˆ«å¼•æ“ (Speech-to-Text)
ä½¿ç”¨ OpenAI Whisper API æˆ–æœ¬åœ° Whisper æ¨¡å‹
"""

import os
import time
import numpy as np
import queue
import threading
from typing import Optional, Callable, List, Tuple
from dataclasses import dataclass
from enum import Enum

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("âš ï¸ OpenAI åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æœ¬åœ° Whisper")

try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False
    print("âš ï¸ Whisper åº“æœªå®‰è£…")


class STTProvider(Enum):
    """STT æä¾›å•†"""
    OPENAI = "openai"
    WHISPER_LOCAL = "whisper_local"


@dataclass
class STTConfig:
    """STT é…ç½®"""
    provider: STTProvider = STTProvider.OPENAI
    model: str = "whisper-1"  # OpenAI æ¨¡å‹
    local_model_size: str = "base"  # æœ¬åœ°æ¨¡å‹: tiny, base, small, medium, large
    language: str = "zh"  # è¯­è¨€ä»£ç 
    sample_rate: int = 16000  # é‡‡æ ·ç‡
    chunk_duration: float = 2.0  # æ¯ä¸ªéŸ³é¢‘å—çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    silence_threshold: float = 0.01  # é™éŸ³é˜ˆå€¼
    min_speech_duration: float = 0.5  # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´
    api_key: Optional[str] = None  # OpenAI API å¯†é’¥


class AudioBuffer:
    """éŸ³é¢‘ç¼“å†²åŒº"""
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self.buffer = []
        self.lock = threading.Lock()
    
    def add_audio(self, audio_data: np.ndarray):
        """æ·»åŠ éŸ³é¢‘æ•°æ®"""
        with self.lock:
            self.buffer.append(audio_data)
    
    def get_audio(self, duration: Optional[float] = None) -> Optional[np.ndarray]:
        """è·å–éŸ³é¢‘æ•°æ®"""
        with self.lock:
            if not self.buffer:
                return None
            
            # åˆå¹¶æ‰€æœ‰éŸ³é¢‘
            audio = np.concatenate(self.buffer)
            
            if duration:
                # è·å–æŒ‡å®šæ—¶é•¿çš„éŸ³é¢‘
                samples_needed = int(duration * self.sample_rate)
                if len(audio) >= samples_needed:
                    result = audio[:samples_needed]
                    # ä¿ç•™å‰©ä½™çš„éŸ³é¢‘
                    self.buffer = [audio[samples_needed:]] if len(audio) > samples_needed else []
                    return result
                else:
                    return None
            else:
                # è·å–æ‰€æœ‰éŸ³é¢‘
                self.buffer.clear()
                return audio
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        with self.lock:
            self.buffer.clear()
    
    def duration(self) -> float:
        """è·å–ç¼“å†²åŒºä¸­éŸ³é¢‘çš„æ€»æ—¶é•¿"""
        with self.lock:
            if not self.buffer:
                return 0.0
            total_samples = sum(len(chunk) for chunk in self.buffer)
            return total_samples / self.sample_rate


class STTEngine:
    """è¯­éŸ³è¯†åˆ«å¼•æ“"""
    
    def __init__(self, config: Optional[STTConfig] = None):
        """
        åˆå§‹åŒ– STT å¼•æ“
        
        Args:
            config: STT é…ç½®
        """
        self.config = config or STTConfig()
        self.running = False
        
        # åˆå§‹åŒ–æä¾›å•†
        if self.config.provider == STTProvider.OPENAI:
            if not HAS_OPENAI:
                print("âš ï¸ OpenAI ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°æœ¬åœ° Whisper")
                self.config.provider = STTProvider.WHISPER_LOCAL
            else:
                # è®¾ç½® OpenAI API
                if self.config.api_key:
                    openai.api_key = self.config.api_key
                elif os.getenv("OPENAI_API_KEY"):
                    openai.api_key = os.getenv("OPENAI_API_KEY")
                else:
                    print("âš ï¸ æœªè®¾ç½® OpenAI API å¯†é’¥")
                    self.config.provider = STTProvider.WHISPER_LOCAL
        
        if self.config.provider == STTProvider.WHISPER_LOCAL:
            if not HAS_WHISPER:
                raise RuntimeError("Whisper åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨è¯­éŸ³è¯†åˆ«")
            # åŠ è½½æœ¬åœ°æ¨¡å‹
            print(f"ğŸ“¥ åŠ è½½ Whisper æ¨¡å‹: {self.config.local_model_size}")
            self.whisper_model = whisper.load_model(self.config.local_model_size)
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = AudioBuffer(self.config.sample_rate)
        
        # è¯†åˆ«ç»“æœé˜Ÿåˆ—
        self.result_queue = queue.Queue()
        
        # å›è°ƒå‡½æ•°
        self.on_transcription: Optional[Callable[[str, float], None]] = None
        
        # å¤„ç†çº¿ç¨‹
        self.process_thread = None
        
        print(f"ğŸ¤ STT å¼•æ“åˆå§‹åŒ–: {self.config.provider.value}")
    
    def start(self):
        """å¯åŠ¨ STT å¼•æ“"""
        if self.running:
            return
        
        self.running = True
        self.process_thread = threading.Thread(target=self._process_loop, daemon=True)
        self.process_thread.start()
        
        print("ğŸ¤ STT å¼•æ“å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢ STT å¼•æ“"""
        self.running = False
        if self.process_thread:
            self.process_thread.join(timeout=2)
        
        print("ğŸ¤ STT å¼•æ“å·²åœæ­¢")
    
    def add_audio(self, audio_data: bytes, format: str = "ulaw"):
        """
        æ·»åŠ éŸ³é¢‘æ•°æ®
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            format: éŸ³é¢‘æ ¼å¼ (ulaw, pcm16)
        """
        # è½¬æ¢ä¸º PCM16
        if format == "ulaw":
            from ..audio import G711Codec
            pcm_data = G711Codec.decode_buffer(audio_data)
            audio_array = np.frombuffer(pcm_data, dtype=np.int16)
        else:
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
        
        # è½¬æ¢é‡‡æ ·ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # å‡è®¾è¾“å…¥æ˜¯ 8kHzï¼Œéœ€è¦è½¬æ¢åˆ° 16kHz
        if format == "ulaw":  # 8kHz
            # ç®€å•çš„ä¸Šé‡‡æ ·ï¼ˆé‡å¤æ ·æœ¬ï¼‰
            audio_array = np.repeat(audio_array, 2)
        
        # å½’ä¸€åŒ–åˆ° [-1, 1]
        audio_float = audio_array.astype(np.float32) / 32768.0
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.add_audio(audio_float)
    
    def _process_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.running:
            # æ£€æŸ¥ç¼“å†²åŒº
            if self.audio_buffer.duration() >= self.config.chunk_duration:
                # è·å–éŸ³é¢‘å—
                audio_chunk = self.audio_buffer.get_audio(self.config.chunk_duration)
                
                if audio_chunk is not None and self._is_speech(audio_chunk):
                    # æ‰§è¡Œè¯­éŸ³è¯†åˆ«
                    start_time = time.time()
                    text = self._transcribe(audio_chunk)
                    duration = time.time() - start_time
                    
                    if text and text.strip():
                        # æ·»åŠ åˆ°ç»“æœé˜Ÿåˆ—
                        self.result_queue.put((text, duration))
                        
                        # è°ƒç”¨å›è°ƒ
                        if self.on_transcription:
                            self.on_transcription(text, duration)
                        
                        print(f"ğŸ¤ è¯†åˆ«: {text} (è€—æ—¶: {duration:.2f}s)")
            
            # çŸ­æš‚ä¼‘çœ 
            time.sleep(0.1)
    
    def _is_speech(self, audio: np.ndarray) -> bool:
        """
        æ£€æµ‹æ˜¯å¦åŒ…å«è¯­éŸ³
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            æ˜¯å¦åŒ…å«è¯­éŸ³
        """
        # è®¡ç®—èƒ½é‡
        energy = np.sqrt(np.mean(audio ** 2))
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™éŸ³é˜ˆå€¼
        return energy > self.config.silence_threshold
    
    def _transcribe(self, audio: np.ndarray) -> Optional[str]:
        """
        æ‰§è¡Œè¯­éŸ³è¯†åˆ«
        
        Args:
            audio: éŸ³é¢‘æ•°æ® (float32, å½’ä¸€åŒ–)
            
        Returns:
            è¯†åˆ«çš„æ–‡æœ¬
        """
        try:
            if self.config.provider == STTProvider.OPENAI:
                return self._transcribe_openai(audio)
            else:
                return self._transcribe_whisper(audio)
        except Exception as e:
            print(f"âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
            return None
    
    def _transcribe_openai(self, audio: np.ndarray) -> Optional[str]:
        """ä½¿ç”¨ OpenAI API è¿›è¡Œè¯­éŸ³è¯†åˆ«"""
        # è½¬æ¢ä¸º int16
        audio_int16 = (audio * 32768).astype(np.int16)
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            import wave
            with wave.open(f.name, 'wb') as wav:
                wav.setnchannels(1)
                wav.setsampwidth(2)
                wav.setframerate(self.config.sample_rate)
                wav.writeframes(audio_int16.tobytes())
            
            temp_path = f.name
        
        try:
            # è°ƒç”¨ OpenAI API
            with open(temp_path, 'rb') as audio_file:
                response = openai.Audio.transcribe(
                    model=self.config.model,
                    file=audio_file,
                    language=self.config.language
                )
            
            return response['text']
        finally:
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
    
    def _transcribe_whisper(self, audio: np.ndarray) -> Optional[str]:
        """ä½¿ç”¨æœ¬åœ° Whisper è¿›è¡Œè¯­éŸ³è¯†åˆ«"""
        # Whisper æœŸæœ› float32 éŸ³é¢‘
        result = self.whisper_model.transcribe(
            audio,
            language=self.config.language,
            fp16=False  # ä½¿ç”¨ FP32ï¼ˆæ›´ç¨³å®šï¼‰
        )
        
        return result.get('text', '')
    
    def get_transcription(self, timeout: float = 0.1) -> Optional[Tuple[str, float]]:
        """
        è·å–è¯†åˆ«ç»“æœ
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            (æ–‡æœ¬, è€—æ—¶) æˆ– None
        """
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def set_callback(self, callback: Callable[[str, float], None]):
        """è®¾ç½®è¯†åˆ«å›è°ƒ"""
        self.on_transcription = callback
    
    def clear_buffer(self):
        """æ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒº"""
        self.audio_buffer.clear()


# ç®€å•çš„ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰
class SimpleVAD:
    """ç®€å•çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
    
    def __init__(self, threshold: float = 0.01, frame_duration: float = 0.02):
        self.threshold = threshold
        self.frame_duration = frame_duration
        self.speech_frames = 0
        self.silence_frames = 0
        self.is_speaking = False
    
    def process(self, audio: np.ndarray, sample_rate: int = 16000) -> bool:
        """
        å¤„ç†éŸ³é¢‘å¸§
        
        Returns:
            æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨
        """
        # è®¡ç®—èƒ½é‡
        energy = np.sqrt(np.mean(audio ** 2))
        
        # æ›´æ–°è®¡æ•°
        if energy > self.threshold:
            self.speech_frames += 1
            self.silence_frames = 0
        else:
            self.silence_frames += 1
            self.speech_frames = 0
        
        # çŠ¶æ€è½¬æ¢
        if not self.is_speaking and self.speech_frames > 10:  # 200ms
            self.is_speaking = True
        elif self.is_speaking and self.silence_frames > 50:  # 1s
            self.is_speaking = False
        
        return self.is_speaking