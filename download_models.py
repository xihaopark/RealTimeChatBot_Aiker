#!/usr/bin/env python3
"""
æ¨¡å‹ä¸‹è½½è„šæœ¬
é¢„å…ˆä¸‹è½½æ‰€éœ€çš„æœ¬åœ°AIæ¨¡å‹
"""

import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import snapshot_download
import torch

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def download_llm_model(model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
    """ä¸‹è½½LLMæ¨¡å‹"""
    print(f"=== Downloading LLM Model: {model_name} ===")
    
    try:
        # åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
        cache_dir = Path.home() / ".cache" / "huggingface"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        print("Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print(f"âœ“ Tokenizer downloaded: {len(tokenizer)} tokens")
        
        print("Downloading model (this may take a while)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        print(f"âœ“ Model downloaded: {model.config}")
        
        # æµ‹è¯•æ¨¡å‹
        print("Testing model...")
        inputs = tokenizer("ä½ å¥½", return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=10)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ“ Model test successful: {response}")
        
        return True
        
    except Exception as e:
        print(f"âœ— Failed to download LLM model: {e}")
        return False


def download_whisper_models():
    """ä¸‹è½½Whisperæ¨¡å‹"""
    print("=== Downloading Whisper Models ===")
    
    try:
        from faster_whisper import WhisperModel
        
        # ä¸‹è½½ä¸åŒå¤§å°çš„æ¨¡å‹
        models = ["tiny", "base", "small"]
        
        for model_size in models:
            print(f"Downloading Whisper {model_size} model...")
            
            model = WhisperModel(
                model_size, 
                device="cuda" if torch.cuda.is_available() else "cpu",
                compute_type="float16" if torch.cuda.is_available() else "int8"
            )
            
            # æµ‹è¯•æ¨¡å‹
            print(f"Testing Whisper {model_size} model...")
            
            print(f"âœ“ Whisper {model_size} model ready")
        
        return True
        
    except Exception as e:
        print(f"âœ— Failed to download Whisper models: {e}")
        return False


def download_tts_models():
    """ä¸‹è½½TTSæ¨¡å‹"""
    print("=== Downloading TTS Models ===")
    
    try:
        # Coqui TTSæ¨¡å‹
        model_name = "tts_models/multilingual/multi-dataset/xtts_v2"
        
        print(f"Downloading TTS model: {model_name}")
        
        # ä½¿ç”¨snapshot_downloadä¸‹è½½æ•´ä¸ªæ¨¡å‹ä»“åº“
        snapshot_download(
            repo_id="coqui/XTTS-v2",
            repo_type="model",
            local_dir=Path.home() / ".cache" / "tts" / "xtts_v2"
        )
        
        print("âœ“ TTS model downloaded")
        return True
        
    except Exception as e:
        print(f"âœ— Failed to download TTS models: {e}")
        print("Note: TTS models will be downloaded on first use")
        return True  # ä¸é˜»å¡å…¶ä»–ä¸‹è½½


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("=== Checking System Requirements ===")
    
    # æ£€æŸ¥CUDA
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"âœ“ CUDA available: {gpu_name} ({gpu_memory:.1f}GB)")
        
        if gpu_memory < 8:
            print("âš ï¸  Warning: GPU memory < 8GB, consider using 4-bit quantization")
    else:
        print("âš ï¸  CUDA not available, will use CPU (slower)")
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    import shutil
    free_space = shutil.disk_usage(".").free / 1024**3
    print(f"Available disk space: {free_space:.1f}GB")
    
    if free_space < 20:
        print("âš ï¸  Warning: Low disk space, models require ~15GB")
        return False
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("Model Download Script for Local AI Phone System\n")
    
    if not check_system_requirements():
        print("âŒ System requirements not met")
        return 1
    
    # é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹
    models_to_download = {
        "LLM (Qwen2.5-7B)": ("qwen", download_llm_model),
        "Whisper STT": ("whisper", download_whisper_models),
        "TTS Models": ("tts", download_tts_models)
    }
    
    print("Available models to download:")
    for i, (name, (key, _)) in enumerate(models_to_download.items(), 1):
        print(f"{i}. {name}")
    print("0. Download all")
    
    try:
        choice = input("\nSelect models to download (0-3): ").strip()
        
        if choice == "0":
            # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
            selected_models = list(models_to_download.values())
        else:
            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(models_to_download):
                selected_models = [list(models_to_download.values())[choice_idx]]
            else:
                print("Invalid choice")
                return 1
    
    except (ValueError, KeyboardInterrupt):
        print("\nDownload cancelled")
        return 0
    
    # ä¸‹è½½é€‰å®šçš„æ¨¡å‹
    successful_downloads = 0
    total_downloads = len(selected_models)
    
    for name, download_func in selected_models:
        if download_func():
            successful_downloads += 1
        print()
    
    print(f"=== Download Summary ===")
    print(f"Successful: {successful_downloads}/{total_downloads}")
    
    if successful_downloads == total_downloads:
        print("ğŸ‰ All models downloaded successfully!")
        print("\nYou can now run: python test_local_ai.py")
        return 0
    else:
        print("âš ï¸  Some downloads failed, but the system may still work")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)