#!/bin/bash

# Llama.cpp è‡ªåŠ¨å®‰è£…è„šæœ¬
# ç¼–è¯‘Llama.cppå¹¶ä¸‹è½½é‡åŒ–æ¨¡åž‹

set -e

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${GREEN}ðŸ”§ Setting up Llama.cpp...${NC}"

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    local deps=("git" "make" "g++" "cmake")
    for dep in "${deps[@]}"; do
        if ! command -v $dep &> /dev/null; then
            echo -e "${RED}âŒ $dep is required but not installed${NC}"
            echo "Please install: sudo apt-get install $dep"
            exit 1
        fi
    done
    echo -e "${GREEN}âœ… Dependencies check passed${NC}"
}

# æ£€æµ‹CUDA
check_cuda() {
    if command -v nvcc &> /dev/null; then
        echo -e "${GREEN}ðŸŽ® CUDA detected, will compile with GPU support${NC}"
        CUDA_ENABLED=1
    else
        echo -e "${YELLOW}âš ï¸  CUDA not found, compiling for CPU only${NC}"
        CUDA_ENABLED=0
    fi
}

check_dependencies
check_cuda

# åˆ›å»ºç›®å½•
mkdir -p llama.cpp/models
cd llama.cpp

# å…‹éš†Llama.cpp
if [ ! -d ".git" ]; then
    echo -e "${YELLOW}Cloning llama.cpp repository...${NC}"
    git clone https://github.com/ggerganov/llama.cpp.git temp_repo
    mv temp_repo/* ./
    mv temp_repo/.git ./
    rm -rf temp_repo
fi

# ç¼–è¯‘
echo -e "${YELLOW}Compiling llama.cpp...${NC}"
mkdir -p build
cd build

if [ $CUDA_ENABLED -eq 1 ]; then
    # ä½¿ç”¨CUDAç¼–è¯‘ (æ–°å‚æ•°)
    cmake .. -DGGML_CUDA=ON -DLLAMA_CURL=OFF
    echo -e "${GREEN}âœ… CMake configured with CUDA support${NC}"
else
    # ä»…CPUç¼–è¯‘
    cmake .. -DLLAMA_CURL=OFF
    echo -e "${GREEN}âœ… CMake configured for CPU${NC}"
fi

cmake --build . --config Release -j$(nproc)
cd ..

# å¤åˆ¶ç¼–è¯‘å¥½çš„æ–‡ä»¶
cp build/bin/llama-server ./server 2>/dev/null || cp build/server ./server 2>/dev/null || cp build/llama-server ./server

# éªŒè¯ç¼–è¯‘ç»“æžœ
if [ ! -f "./server" ]; then
    echo -e "${RED}âŒ Compilation failed - server executable not found${NC}"
    echo "Available files in build:"
    find build -name "*server*" -o -name "llama*" | head -10
    exit 1
fi

chmod +x ./server

echo -e "${GREEN}âœ… Llama.cpp compiled successfully${NC}"

# ä¸‹è½½æ¨¡åž‹
echo -e "${YELLOW}Downloading Qwen2.5-7B-Instruct GGUF model...${NC}"
cd models

# ä½¿ç”¨è¾ƒå°çš„Q4_K_Mé‡åŒ–ç‰ˆæœ¬ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡
MODEL_NAME="Qwen2.5-7B-Instruct-Q4_K_M.gguf"
MODEL_URL="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/${MODEL_NAME}"

if [ ! -f "$MODEL_NAME" ]; then
    echo -e "${YELLOW}Downloading $MODEL_NAME...${NC}"
    wget -O "$MODEL_NAME" "$MODEL_URL" || {
        echo -e "${RED}âŒ Model download failed${NC}"
        echo "You can manually download from: $MODEL_URL"
        exit 1
    }
    echo -e "${GREEN}âœ… Model downloaded: $MODEL_NAME${NC}"
else
    echo -e "${GREEN}âœ… Model already exists: $MODEL_NAME${NC}"
fi

cd ..

# åˆ›å»ºå¯åŠ¨è„šæœ¬
cat > start_server.sh << 'EOF'
#!/bin/bash

# Llama.cpp Server å¯åŠ¨è„šæœ¬

MODEL_PATH="./models/Qwen2.5-7B-Instruct-Q4_K_M.gguf"
HOST="127.0.0.1"
PORT="8080"
CTX_SIZE="4096"
THREADS=$(nproc)

# æ£€æµ‹GPU
if command -v nvidia-smi &> /dev/null && [ -f "./server" ]; then
    GPU_LAYERS=35  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    echo "Starting Llama.cpp server with GPU acceleration..."
    ./server \
        -m "$MODEL_PATH" \
        -c "$CTX_SIZE" \
        --host "$HOST" \
        --port "$PORT" \
        -t "$THREADS" \
        -ngl "$GPU_LAYERS" \
        --log-disable
else
    echo "Starting Llama.cpp server (CPU only)..."
    ./server \
        -m "$MODEL_PATH" \
        -c "$CTX_SIZE" \
        --host "$HOST" \
        --port "$PORT" \
        -t "$THREADS" \
        --log-disable
fi
EOF

chmod +x start_server.sh

# æµ‹è¯•å¯åŠ¨
echo -e "${YELLOW}Testing server startup...${NC}"
timeout 10s ./start_server.sh &
SERVER_PID=$!

sleep 5

# æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨
if curl -s http://127.0.0.1:8080/health > /dev/null; then
    echo -e "${GREEN}âœ… Server test successful${NC}"
    kill $SERVER_PID 2>/dev/null || true
else
    echo -e "${YELLOW}âš ï¸  Server test inconclusive (this is normal)${NC}"
    kill $SERVER_PID 2>/dev/null || true
fi

echo -e "${GREEN}ðŸŽ‰ Llama.cpp setup completed!${NC}"
echo ""
echo "To start the server:"
echo "  cd services/llama.cpp && ./start_server.sh"
echo ""
echo "Server will be available at: http://127.0.0.1:8080"