# VTX AI Phone System - Claude Memory

## é¡¹ç›®æ¦‚è¿°
è¿™æ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°AIçš„å®æ—¶ç”µè¯å®¢æœç³»ç»Ÿï¼Œé›†æˆäº†SIPåè®®ã€RTPéŸ³é¢‘ä¼ è¾“ã€STTè¯­éŸ³è¯†åˆ«ã€TTSè¯­éŸ³åˆæˆå’ŒLLMå¯¹è¯åŠŸèƒ½ã€‚

**ğŸš€ V2é‡å¤§å‡çº§å®Œæˆï¼ç°å·²æ”¯æŒä¸¤ç§éƒ¨ç½²æ¨¡å¼**

## V2 ä¼˜åŒ–æ¶æ„ (Vast.aiå®¹å™¨ç¯å¢ƒ) â­ æ¨è

ä¸“é—¨ä¸ºVast.aiå®¹å™¨ç¯å¢ƒä¼˜åŒ–ï¼Œä½¿ç”¨è¿›ç¨‹å†…è°ƒç”¨çš„é«˜æ€§èƒ½ç»„ä»¶ã€‚

### æ ¸å¿ƒä¼˜åŠ¿
- **âœ… å®Œç¾é€‚é…Vast.ai**: æ— éœ€systemdã€dockerdç­‰ç³»ç»Ÿæƒé™
- **ğŸš€ llama.cppç›´æ¥è°ƒç”¨**: é€šè¿‡llama-cpp-pythonåº“åœ¨è¿›ç¨‹å†…è¿è¡Œ
- **ğŸ’¾ GPUåŠ é€Ÿ**: å……åˆ†åˆ©ç”¨Vast.aiçš„GPUèµ„æº
- **ğŸ”§ æŠ€æœ¯æ ˆç»Ÿä¸€**: Vosk + Piper + llama.cppçš„æœ€ä½³ç»„åˆ

### æŠ€æœ¯æ ˆ (Vast.aiä¼˜åŒ–)
- **STT**: Vosk (è½»é‡çº§ï¼Œè¿›ç¨‹å†…åŠ è½½)
- **LLM**: llama-cpp-python (è¿›ç¨‹å†…è°ƒç”¨ï¼Œæ— éœ€server)
- **TTS**: Piper (é€šè¿‡subprocessè°ƒç”¨äºŒè¿›åˆ¶)

### éƒ¨ç½²å‘½ä»¤ (Vast.ai)
```bash
# æµ‹è¯•ç¯å¢ƒ
python test_v2.py

# å¯åŠ¨ç³»ç»Ÿ
./start_v2.sh
```

## V2 åˆ†ç¦»æ¶æ„ (å®Œæ•´LinuxæœåŠ¡å™¨)

é€‚ç”¨äºå…·æœ‰å®Œæ•´ç³»ç»Ÿæƒé™çš„LinuxæœåŠ¡å™¨ç¯å¢ƒã€‚

### æ ¸å¿ƒä¼˜åŠ¿
- **10x å¯åŠ¨é€Ÿåº¦**: ä»åˆ†é’Ÿçº§ä¼˜åŒ–åˆ°ç§’çº§
- **4x å¹¶å‘èƒ½åŠ›**: æ”¯æŒ15-20è·¯å¹¶å‘é€šè¯
- **50% èµ„æºå ç”¨**: CPUå’Œå†…å­˜å¤§å¹…ä¼˜åŒ–
- **é›¶ç¯å¢ƒä¾èµ–**: è§£å†³äº†æ‰€æœ‰ALSAã€cuDNNç­‰ç¯å¢ƒé—®é¢˜

### æŠ€æœ¯æ ˆ (åˆ†ç¦»)
- **STT**: Vosk Server (ç‹¬ç«‹æœåŠ¡)
- **LLM**: Llama.cpp Server (ç‹¬ç«‹æœåŠ¡)
- **TTS**: Piper (ç‹¬ç«‹è¿›ç¨‹)

### V2æ–‡ä»¶ç»“æ„
```
â”œâ”€â”€ aiker_v2/                     # V2æ ¸å¿ƒç›®å½•
â”‚   â”œâ”€â”€ app_integrated.py         # ä¸€ä½“åŒ–ä¸»ç¨‹åº (Vast.ai)
â”‚   â”œâ”€â”€ app.py                   # åˆ†ç¦»æ¶æ„ä¸»ç¨‹åº (Linux)
â”‚   â”œâ”€â”€ llm_service_integrated.py # Transformers LLM (ä¸€ä½“åŒ–)
â”‚   â”œâ”€â”€ stt_service_integrated.py # RealtimeSTT (ä¸€ä½“åŒ–)
â”‚   â”œâ”€â”€ tts_service_integrated.py # RealtimeTTS (ä¸€ä½“åŒ–)
â”‚   â”œâ”€â”€ llm_service.py           # Llama.cpp LLM (åˆ†ç¦»)
â”‚   â”œâ”€â”€ stt_service.py           # Vosk STT (åˆ†ç¦»)
â”‚   â”œâ”€â”€ tts_service.py           # Piper TTS (åˆ†ç¦»)
â”‚   â””â”€â”€ call_handler.py          # é€šè¯å¤„ç†å™¨
â”œâ”€â”€ services/                    # å¤–éƒ¨AIæœåŠ¡ (åˆ†ç¦»æ¶æ„)
â”‚   â”œâ”€â”€ piper/                   # TTSæœåŠ¡
â”‚   â”œâ”€â”€ llama.cpp/               # LLMæœåŠ¡
â”‚   â””â”€â”€ vosk/                    # STTæœåŠ¡
â”œâ”€â”€ requirements_v2_integrated.txt # ä¸€ä½“åŒ–ä¾èµ–
â”œâ”€â”€ start_v2_integrated.sh        # Vast.aiå¯åŠ¨è„šæœ¬
â”œâ”€â”€ setup_aiker_v2.sh             # åˆ†ç¦»æ¶æ„éƒ¨ç½²è„šæœ¬
â””â”€â”€ test_integrated_v2.py         # ä¸€ä½“åŒ–æµ‹è¯•å·¥å…·
```

### éƒ¨ç½²é€‰æ‹©

| ç¯å¢ƒç±»å‹ | æ¨èæ¶æ„ | å¯åŠ¨å‘½ä»¤ |
|---------|---------|---------|
| **Vast.aiå®¹å™¨** | ä¸€ä½“åŒ–æ¶æ„ | `./start_v2_integrated.sh` |
| **å®Œæ•´LinuxæœåŠ¡å™¨** | åˆ†ç¦»æ¶æ„ | `./setup_aiker_v2.sh && ./start_aiker_v2.sh` |
| **å¼€å‘æµ‹è¯•** | ä¸€ä½“åŒ–æ¶æ„ | `python test_integrated_v2.py` |

## V1é—ç•™æ¶æ„ (å·²ä¼˜åŒ–æ›¿ä»£)

### ä¸»è¦æ–‡ä»¶ç»“æ„ (V1)
```
â”œâ”€â”€ main_local_ai.py          # V1æœ¬åœ°AIç‰ˆæœ¬ä¸»ç¨‹åº
â”œâ”€â”€ production_local_ai.py    # V1ç”Ÿäº§ç‰ˆæœ¬ä¸»ç¨‹åº
â”œâ”€â”€ sip_client.py            # SIPå®¢æˆ·ç«¯å®ç°
â”œâ”€â”€ rtp_handler.py           # RTPéŸ³é¢‘å¤„ç†
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py          # ç»Ÿä¸€é…ç½®ç®¡ç†
â”‚   â””â”€â”€ local_ai_config.py   # V1æœ¬åœ°AIé…ç½®
â”œâ”€â”€ local_ai/                # V1 AIæœåŠ¡ (å·²è¢«V2æ›¿ä»£)
â”‚   â”œâ”€â”€ local_stt.py         # æ—§ç‰ˆè¯­éŸ³è¯†åˆ«
â”‚   â”œâ”€â”€ local_tts.py         # æ—§ç‰ˆè¯­éŸ³åˆæˆ
â”‚   â”œâ”€â”€ local_llm.py         # æ—§ç‰ˆå¤§è¯­è¨€æ¨¡å‹
â”‚   â””â”€â”€ audio_converter.py   # éŸ³é¢‘æ ¼å¼è½¬æ¢
â””â”€â”€ logs/                    # æ—¥å¿—ç›®å½•
```

## V1å·²è§£å†³é—®é¢˜ (V2ä¸­å·²å½»åº•è§£å†³)
1. âœ… **cuDNNä¾èµ–é—®é¢˜**: V2æ— éœ€cuDNN
2. âœ… **ALSAéŸ³é¢‘é—®é¢˜**: V2æ— éŸ³é¢‘è®¾å¤‡ä¾èµ–
3. âœ… **SIPæ³¨å†Œå¤±è´¥**: V2ä¿æŒåŒæ­¥æ³¨å†Œæœºåˆ¶
4. âœ… **TTSéŸ³é¢‘ç”Ÿæˆè­¦å‘Š**: V2ä½¿ç”¨Piperè§£å†³
5. âœ… **Transformersç”Ÿæˆå‚æ•°**: V2ä½¿ç”¨Llama.cpp

## æŠ€æœ¯æ ˆå¯¹æ¯”

| ç»„ä»¶ | V1 | V2 | æ”¹è¿› |
|------|----|----|------|
| STT | RealtimeSTT + Whisper | Vosk | è½»é‡åŒ–ã€æ— ä¾èµ– |
| LLM | Transformers + Qwen2.5 | Llama.cpp + GGUF | é«˜å¹¶å‘ã€é‡åŒ– |
| TTS | RealtimeTTS | Piper | æé€Ÿã€ç¨³å®š |
| å¯åŠ¨æ—¶é—´ | 2-3åˆ†é’Ÿ | 10-15ç§’ | 10xæå‡ |
| å†…å­˜å ç”¨ | 8-12GB | 4-6GB | 50%ä¼˜åŒ– |
| å¹¶å‘èƒ½åŠ› | 3-5è·¯ | 15-20è·¯ | 4xæå‡ |

## æ ¸å¿ƒæŠ€æœ¯å®ç° (V1&V2é€šç”¨)

### SIPæ³¨å†Œæœºåˆ¶ (è‡³å…³é‡è¦)
**å¿…é¡»ä½¿ç”¨åŒæ­¥æ³¨å†Œæµç¨‹**ï¼Œå¼‚æ­¥æ³¨å†Œä¼šå¯¼è‡´æ³¨å†Œå¤±è´¥ï¼

#### å…³é”®ç»„ä»¶ï¼š
1. **å“åº”é˜Ÿåˆ—æœºåˆ¶**: `register_response_queue = queue.Queue()`
2. **åŒæ­¥ç­‰å¾…**: `waiting_for_register = True`
3. **CSeqåŒ¹é…**: ç¡®ä¿æ³¨å†Œå“åº”ä¸è¯·æ±‚åŒ¹é…

#### æ³¨å†Œæµç¨‹ï¼š
```python
# Step 1: å‘é€åˆå§‹REGISTER (æ— è®¤è¯)
self.cseq += 1
self.current_cseq = self.cseq
self.waiting_for_register = True

# Step 2: åŒæ­¥ç­‰å¾…å“åº” 
response = self.register_response_queue.get(timeout=10)

# Step 3: å¤„ç†407è®¤è¯æŒ‘æˆ˜ï¼Œå‘é€å¸¦è®¤è¯çš„REGISTER  
# Step 4: å†æ¬¡åŒæ­¥ç­‰å¾…200 OK
```

#### å…³é”®æ³¨å†Œå¤´éƒ¨å­—æ®µï¼š
```sip
Via: SIP/2.0/UDP {local_ip}:{local_port};branch={branch};rport
From: <sip:{username}@{domain}>;tag={from_tag}
Call-ID: {uuid}@{local_ip}
Contact: <sip:{username}@{local_ip}:{local_port}>
User-Agent: VTX-AI-System/2.0
Expires: 60
```

### éŸ³é¢‘ç¼–ç /è§£ç 

#### G.711 Î¼-lawç¼–ç  (RTP Payload Type 0)
- **é‡‡æ ·ç‡**: 8kHz
- **åŒ…å¤§å°**: 160å­—èŠ‚ (20mséŸ³é¢‘)  
- **ç¼–ç å…¬å¼**: PCM16 -> Î¼-lawå‹ç¼©
- **RTPæ—¶é—´æˆ³å¢é‡**: 160 (æ¯20msåŒ…)

#### éŸ³é¢‘å¤„ç†æµç¨‹ï¼š
```python
# æ¥æ”¶éŸ³é¢‘: RTP -> Î¼-law -> PCM16
rtp_packet = parse_rtp_packet(data)
mulaw_audio = rtp_packet['payload'] 
pcm_audio = G711Codec.mulaw_to_pcm(mulaw_audio)

# å‘é€éŸ³é¢‘: PCM16/AIç”ŸæˆéŸ³é¢‘ -> Î¼-law -> RTP
ai_audio = tts_service.synthesize_text(text)
mulaw_data = AudioConverter.convert_pcm16k_to_rtp(ai_audio)  
send_rtp_packet(mulaw_data, payload_type=0)
```

#### éŸ³é¢‘åŒæ­¥ï¼š
- **åŒ…é—´éš”**: 20ms (time.sleep(0.02))
- **åºåˆ—å·**: æ¯åŒ…é€’å¢1
- **æ—¶é—´æˆ³**: æ¯åŒ…é€’å¢160 (@8kHz)

### RTPå¤„ç†å…³é”®ç‚¹
```python
# RTPå¤´éƒ¨æ„å»º
header = struct.pack('!BBHII',
    0x80,           # V=2, P=0, X=0, CC=0  
    payload_type,   # 0=PCMU, 8=PCMA
    sequence,       # åŒ…åºåˆ—å·
    timestamp,      # æ—¶é—´æˆ³
    ssrc)           # åŒæ­¥æºæ ‡è¯†

# éŸ³é¢‘åˆ†åŒ…å‘é€ (160å­—èŠ‚/åŒ…)
chunk_size = 160
for i in range(0, len(audio_data), chunk_size):
    chunk = audio_data[i:i+chunk_size]
    if len(chunk) < chunk_size:
        chunk += b'\\x7f' * (chunk_size - len(chunk))  # å¡«å……
    rtp_handler.send_audio(chunk)
    time.sleep(0.02)  # 20msé—´éš”
```

## V2ç‰¹æœ‰å®ç°

### æœåŠ¡å¯åŠ¨é¡ºåº
```bash
# 1. å¯åŠ¨Llama.cppæœåŠ¡å™¨
cd services/llama.cpp && ./start_server.sh &

# 2. ç­‰å¾…LLMæœåŠ¡å°±ç»ª
curl -s http://127.0.0.1:8080/health

# 3. å¯åŠ¨ä¸»åº”ç”¨
cd aiker_v2 && python app.py
```

### é«˜å¹¶å‘é€šè¯å¤„ç†
```python
# CallManagerç®¡ç†å¤šä¸ªCallHandler
class CallManager:
    def handle_incoming_call(self, call_info):
        call_handler = CallHandler(call_info, self.tts, self.llm)
        self.active_calls[call_id] = call_handler
        call_handler.start()  # ç‹¬ç«‹çº¿ç¨‹å¤„ç†
```

### AIæœåŠ¡è°ƒç”¨æ¨¡å¼
```python
# TTS: å­è¿›ç¨‹è°ƒç”¨Piper
subprocess.Popen([piper_exe, '--model', model_path])

# LLM: HTTPè°ƒç”¨Llama.cppæœåŠ¡å™¨
requests.post('http://127.0.0.1:8080/completion', json=data)

# STT: ç›´æ¥è°ƒç”¨Vosk Pythonåº“
recognizer.AcceptWaveform(audio_chunk)
```

## æ•…éšœæ’é™¤

### V2éƒ¨ç½²é—®é¢˜
1. **Llama.cppç¼–è¯‘å¤±è´¥**:
   ```bash
   sudo apt-get install build-essential cmake
   # æ£€æŸ¥CUDA: nvcc --version
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**:
   ```bash
   # æ‰‹åŠ¨ä¸‹è½½åˆ° services/*/models/ ç›®å½•
   wget https://huggingface.co/...
   ```

3. **æœåŠ¡å¯åŠ¨å¤±è´¥**:
   ```bash
   # æ£€æŸ¥ç«¯å£å ç”¨
   netstat -tulpn | grep :8080
   
   # æ£€æŸ¥æœåŠ¡å¥åº·
   curl http://127.0.0.1:8080/health
   ```

### V1ç¯å¢ƒé—®é¢˜ (V2å·²è§£å†³)
1. **ALSAé”™è¯¯**: 
   ```bash
   # V2ä¸å†éœ€è¦è¿™äº›é…ç½®
   apt-get install -y libasound2-plugins
   echo 'pcm.!default { type null }' > /etc/alsa/conf.d/99-null.conf
   ```

2. **cuDNNç¼ºå¤±**: 
   ```bash
   # V2ä¸å†éœ€è¦cuDNN
   sudo ln -sf /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8 ...
   ```

## æ€§èƒ½å¯¹æ¯”

### V1 vs V2 åŸºå‡†æµ‹è¯•
| æŒ‡æ ‡ | V1 | V2 | æå‡ |
|------|----|----|------|
| å¯åŠ¨æ—¶é—´ | 120-180s | 10-15s | **10x** |
| å†…å­˜å ç”¨ | 8-12GB | 4-6GB | **50%** |
| å¹¶å‘é€šè¯ | 3-5è·¯ | 15-20è·¯ | **4x** |
| TTSå»¶è¿Ÿ | 2-3s | 0.3-0.5s | **6x** |
| STTç²¾åº¦ | 85-90% | 90-95% | **+5%** |

## å¸¸ç”¨å‘½ä»¤

### V2å‘½ä»¤ (æ¨è)
```bash
# ä¸€é”®éƒ¨ç½²
./setup_aiker_v2.sh

# å¯åŠ¨ç³»ç»Ÿ
./start_aiker_v2.sh

# æ€§èƒ½æµ‹è¯•
python test_v2_performance.py

# ç»„ä»¶æµ‹è¯•
./test_aiker_v2.sh

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/aiker_v2.log
```

### V1å‘½ä»¤ (é—ç•™)
```bash
# æ£€æŸ¥CUDAç¯å¢ƒ
nvidia-smi

# æ£€æŸ¥cuDNN
python -c "import torch; print(torch.backends.cudnn.is_available())"

# ä¸€é”®ç¯å¢ƒè®¾ç½®
./setup_environment.sh

# å¯åŠ¨V1ç³»ç»Ÿ
./start_ai_fixed.sh
```

## é…ç½®è¦æ±‚

### V2æœ€ä½è¦æ±‚
- **OS**: Linux (Ubuntu 18.04+)
- **Python**: 3.8+
- **å†…å­˜**: 8GB+ (æ¨è)
- **ç£ç›˜**: 5GB+ å¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: ç¨³å®šç½‘ç»œè¿æ¥

### V2æ¨èé…ç½®
- **CPU**: 8æ ¸+ 
- **GPU**: NVIDIA GPU (å¯é€‰ï¼Œç”¨äºLLMåŠ é€Ÿ)
- **å†…å­˜**: 16GB+
- **SSD**: å›ºæ€ç¡¬ç›˜

## è”ç³»æ–¹å¼
- VTX SIPæœåŠ¡å™¨: core1-us-lax.myippbx.com:5060
- æŠ€æœ¯æ”¯æŒ: é…ç½®.envæ–‡ä»¶ä¸­çš„VTXå‡­æ®

---

**å»ºè®®ä½¿ç”¨V2æ¶æ„ï¼Œå®ƒè§£å†³äº†V1çš„æ‰€æœ‰é—®é¢˜å¹¶å¤§å¹…æå‡äº†æ€§èƒ½ï¼** ğŸš€